install.packages("lubridate")
install.packages("lubridate")
install.packages("dplyr")
library(lubridate) #pacote para trabalhar com datas
library(dplyr)  #pacote para tratamento de dados
data_final <- Sys.Date() #usa a data do sistema como referência
datas <- seq(data_final %m-% months(59), data_final, by = "month") #cria uma sequência de 24 meses, terminando no mês atual
data_final
datas
refs <- format(datas, "%Y%m") #converte de "yyyy-mm-dd" para "yyyymm"
refs
dados_lista <- list() #lista para guardar os arquivos csv
#looping para baixar e descompactar os arquivos
for (ref in refs){ #realiza os comandos abaixo para cada data no objeto refs
url <- paste0("https://dados.cvm.gov.br/dados/FI/DOC/INF_DIARIO/DADOS/inf_diario_fi_", ref, ".zip") #cola a data no link para download
arq_zip <- tempfile(fileext = ".zip") #arquivo temporário para o compactado
arq_dir <- tempdir() #arquivo temporário para o descompactado
download.file(url, arq_zip, mode = "wb", quiet = TRUE) #baixando a url criada no arquivo temporário
unzip(arq_zip, exdir = arq_dir) #descompactando o arquivo temporário
arq_csv <- list.files(arq_dir, pattern = paste0("inf_diario_fi_", ref, ".csv$"), full.names = TRUE) #dentro do arquivo descompactado procura aquele que possui esse padrão
dados <- read.csv(arq_csv, sep = ";", stringsAsFactors = FALSE) #lê o arquivo zipado
#tratando os meses em que as colunas estão com nomes diferentes
if ("TP_FUNDO_CLASSE" %in% names(dados)) {
names(dados)[names(dados) == "TP_FUNDO_CLASSE"] <- "TP_FUNDO"
}
#tratando os meses em que as colunas estão com nomes diferentes
if ("CNPJ_FUNDO_CLASSE" %in% names(dados)) {
names(dados)[names(dados) == "CNPJ_FUNDO_CLASSE"] <- "CNPJ_FUNDO"
}
dados$referencia <- ref #criando uma coluna com o mês de referência
dados_lista[[ref]] <- dados #incluíndo os dados na lista
}
#looping para baixar e descompactar os arquivos
for (ref in refs){ #realiza os comandos abaixo para cada data no objeto refs
url <- paste0("https://dados.cvm.gov.br/dados/FI/DOC/INF_DIARIO/DADOS/inf_diario_fi_", ref, ".zip") #cola a data no link para download
arq_zip <- tempfile(fileext = ".zip") #arquivo temporário para o compactado
arq_dir <- tempdir() #arquivo temporário para o descompactado
download.file(url, arq_zip, mode = "wb", quiet = TRUE) #baixando a url criada no arquivo temporário
unzip(arq_zip, exdir = arq_dir) #descompactando o arquivo temporário
arq_csv <- list.files(arq_dir, pattern = paste0("inf_diario_fi_", ref, ".csv$"), full.names = TRUE) #dentro do arquivo descompactado procura aquele que possui esse padrão
dados <- read.csv(arq_csv, sep = ";", stringsAsFactors = FALSE) #lê o arquivo zipado
#tratando os meses em que as colunas estão com nomes diferentes
if ("TP_FUNDO_CLASSE" %in% names(dados)) {
names(dados)[names(dados) == "TP_FUNDO_CLASSE"] <- "TP_FUNDO"
}
#tratando os meses em que as colunas estão com nomes diferentes
if ("CNPJ_FUNDO_CLASSE" %in% names(dados)) {
names(dados)[names(dados) == "CNPJ_FUNDO_CLASSE"] <- "CNPJ_FUNDO"
}
dados$referencia <- ref #criando uma coluna com o mês de referência
dados_lista[[ref]] <- dados #incluíndo os dados na lista
}
data_final <- Sys.Date() #usa a data do sistema como referência
datas <- seq(data_final %m-% months(23), data_final, by = "month") #cria uma sequência de 24 meses, terminando no mês atual
refs <- format(datas, "%Y%m") #converte de "yyyy-mm-dd" para "yyyymm"
dados_lista <- list() #lista para guardar os arquivos csv
#looping para baixar e descompactar os arquivos
for (ref in refs){ #realiza os comandos abaixo para cada data no objeto refs
url <- paste0("https://dados.cvm.gov.br/dados/FI/DOC/INF_DIARIO/DADOS/inf_diario_fi_", ref, ".zip") #cola a data no link para download
arq_zip <- tempfile(fileext = ".zip") #arquivo temporário para o compactado
arq_dir <- tempdir() #arquivo temporário para o descompactado
download.file(url, arq_zip, mode = "wb", quiet = TRUE) #baixando a url criada no arquivo temporário
unzip(arq_zip, exdir = arq_dir) #descompactando o arquivo temporário
arq_csv <- list.files(arq_dir, pattern = paste0("inf_diario_fi_", ref, ".csv$"), full.names = TRUE) #dentro do arquivo descompactado procura aquele que possui esse padrão
dados <- read.csv(arq_csv, sep = ";", stringsAsFactors = FALSE) #lê o arquivo zipado
#tratando os meses em que as colunas estão com nomes diferentes
if ("TP_FUNDO_CLASSE" %in% names(dados)) {
names(dados)[names(dados) == "TP_FUNDO_CLASSE"] <- "TP_FUNDO"
}
#tratando os meses em que as colunas estão com nomes diferentes
if ("CNPJ_FUNDO_CLASSE" %in% names(dados)) {
names(dados)[names(dados) == "CNPJ_FUNDO_CLASSE"] <- "CNPJ_FUNDO"
}
dados$referencia <- ref #criando uma coluna com o mês de referência
dados_lista[[ref]] <- dados #incluíndo os dados na lista
}
dados_lista[[1]]
dados_lista[[2]]
dados_completos <- bind_rows(dados_lista) #empilhando os dados da lista usando dplyr
View(dados_completos)
dados_completos$CNPJ_FUNDO
#quantidade de fundos na base em todo o período (sem filtrar) = 31555
length(unique(dados_completos$CNPJ_FUNDO)) #tamanho do vetor de cnpj (sem repetições)
dados_completos$DT_COMPTC
class(dados_completos$DT_COMPTC)
#transformar a coluna de data em Date para conseguir manipular
dados_completos$DT_COMPTC <- as.Date(dados_completos$DT_COMPTC)
class(dados_completos$DT_COMPTC)
#calcular a data de corte = 365 dias corridos a menos
data_corte <- max(dados_completos$DT_COMPTC) %m-% days(365)
#fundos que possuem pelo menos 252 observações (dias úteis) desde a data de corte
cnpjs_filtrados1 <- dados_completos %>%
filter(DT_COMPTC >= data_corte) %>%
group_by(CNPJ_FUNDO) %>%
summarise(n_obs = sum(!is.na(VL_QUOTA))) %>%
filter(n_obs >= 252) %>%
pull(CNPJ_FUNDO)
cnpjs_filtrados1
cnpjs_filtrados1[1]
#### FILTRANDO CNPJS COM MAIS DE 100 COTISTAS (RETIRA FUNDOS EXCLUSIVOS E POTENCIALMENTE FECHADOS)
cnpjs_filtrados2 <- dados_completos %>% #esse símbolo significa que vai fazer uma sequência de operações
group_by(CNPJ_FUNDO) %>% #opera por CNPJ
summarise(media_cotistas = mean(NR_COTST, na.rm = TRUE)) %>% #tira a média de cotistas no período por CNPJ
filter(media_cotistas > 100)  %>% #filtra apenas CNPJs com mais de 100 cotistas em média
pull(CNPJ_FUNDO) #puxa apenas o cnpj no objeto final
cnpjs_filtrados2
#aplicando os filtros
dados_filtrados <- dados_completos %>%
filter(CNPJ_FUNDO %in% cnpjs_filtrados1,
CNPJ_FUNDO %in% cnpjs_filtrados2)
View(dados_filtrados)
dados_filtrados[,dados_filtrados$CNPJ_FUNDO=="04.621.018/0001-61"]
dados_filtrados[dados_filtrados$CNPJ_FUNDO=="04.621.018/0001-61",]
dados_completos[,dados_completos$CNPJ_FUNDO=="04.621.018/0001-61"]
View(dados_filtrados)
View(dados_filtrados)
View(dados_filtrados)
#quantidade de fundos na base filtrada = 1029
length(unique(dados_filtrados$CNPJ_FUNDO))
indicadores <- dados_filtrados %>%
arrange(CNPJ_FUNDO, DT_COMPTC) %>% #organiza por CNPJ e data
group_by(CNPJ_FUNDO) %>% #realizar as operações abaixo para cada CNPJ
mutate( #cria colunas
retorno_diario = (VL_QUOTA / lag(VL_QUOTA)) - 1,
ano_atual = year(DT_COMPTC),
mes_atual = floor_date(DT_COMPTC, "month")
) %>%
summarise( #cria os indicadores para cada CNPJ
#retorno mediário médio no período inteiro
retorno_diario_medio = 100*mean(retorno_diario, na.rm = TRUE),
#retorno no último mês
retorno_1m = 100*(
last(VL_QUOTA, order_by = DT_COMPTC)/
first(na.omit(VL_QUOTA[DT_COMPTC>=(max(DT_COMPTC) %m-% months(1))]))-1),
#retorno no último ano
retorno_1a = 100*(
last(VL_QUOTA, order_by = DT_COMPTC)/
first(na.omit(VL_QUOTA[DT_COMPTC >= (max(DT_COMPTC) %m-% years(1))]))-1),
#retorno nos últimos 2 anos
retorno_2a = 100*(
last(VL_QUOTA, order_by = DT_COMPTC)/
first(na.omit(VL_QUOTA), order_by = DT_COMPTC)-1),
#volatilidade no último mês anualizada
vol_1m = sd(retorno_diario[DT_COMPTC >= (max(DT_COMPTC) %m-% months(1))],na.rm = TRUE)*sqrt(252),
#volatilidade no último ano anualizada
vol_1a=sd(retorno_diario[DT_COMPTC>=(max(DT_COMPTC) %m-% years(1))],na.rm = TRUE)*sqrt(252),
#volatilidade nos últimos 2 anos anualizada
vol_2a=sd(retorno_diario, na.rm=TRUE)*sqrt(2*252),
#patrimônio líquido médio
pl_medio = mean(VL_PATRIM_LIQ, na.rm = TRUE)
)
View(indicadores)
#fundo com maior retorno no último mês
head(indicadores %>% arrange(desc(retorno_1m)),1)
#fundo com menor retorno no último mês
head(indicadores %>% arrange(retorno_1m),1)
#fundo com menor volatilidade no último ano
head(indicadores %>% arrange(vol_1a),1)
#resumo dos indicadores
summary(indicadores)
install.packages("cluster")
install.packages("factoextra")
library(cluster)
library(factoextra)
# Seleciona apenas as colunas numéricas dos indicadores
dados_cluster <- indicadores %>%
select(retorno_diario_medio, retorno_1m, retorno_1a, retorno_2a,
vol_1m, vol_1a, vol_2a, pl_medio)
View(dados_cluster)
# Padroniza as variáveis (média = 0, desvio padrão = 1)
dados_cluster_scaled <- scale(dados_cluster)
View(dados_cluster_scaled)
# Define número de clusters (k) - você pode usar o método do cotovelo
fviz_nbclust(dados_cluster_scaled, kmeans, method = "wss") +
theme_minimal()
# Remover linhas com NA/NaN/Inf
dados_cluster_scaled <- dados_cluster_scaled[complete.cases(dados_cluster_scaled), ]
# Remover colunas com variância zero (constantes)
variancias <- apply(dados_cluster, 2, var, na.rm = TRUE)
dados_cluster <- dados_cluster[, variancias > 0]
#Substituir NA por zero ou pela média (se fizer sentido no seu caso)
dados_cluster[is.na(dados_cluster)] <- 0
# Define número de clusters (k) - você pode usar o método do cotovelo
fviz_nbclust(dados_cluster_scaled, kmeans, method = "wss") +
theme_minimal()
# Remover colunas com variância zero (constantes)
variancias <- apply(dados_cluster_scaled, 2, var, na.rm = TRUE)
dados_cluster_scaled <- dados_cluster_scaled[, variancias > 0]
#Substituir NA por zero ou pela média (se fizer sentido no seu caso)
dados_cluster_scaled[is.na(dados_cluster_scaled)] <- 0
# Define número de clusters (k) - você pode usar o método do cotovelo
fviz_nbclust(dados_cluster_scaled, kmeans, method = "wss") +
theme_minimal()
# Seleciona apenas as colunas numéricas dos indicadores
dados_cluster <- indicadores %>%
select(retorno_diario_medio, retorno_1m, retorno_1a, retorno_2a,
vol_1m, vol_1a, vol_2a, pl_medio)
# Padroniza as variáveis (média = 0, desvio padrão = 1)
dados_cluster_scaled <- scale(dados_cluster)
# Remover linhas com NA/NaN/Inf
dados_cluster_scaled <- dados_cluster_scaled[complete.cases(dados_cluster_scaled), ]
# Padroniza as variáveis (média = 0, desvio padrão = 1)
dados_cluster_scaled <- scale(dados_cluster)
# Padroniza as variáveis (média = 0, desvio padrão = 1)
dados_cluster_scaled <- scale(dados_cluster)
# Define número de clusters (k) - você pode usar o método do cotovelo
fviz_nbclust(dados_cluster_scaled, kmeans, method = "wss") +
theme_minimal()
# Remove colunas sem variabilidade
variancias <- apply(dados_cluster, 2, var, na.rm = TRUE)
dados_cluster <- dados_cluster[, variancias > 0]
# Substitui NA por zero (ou média, se preferir)
dados_cluster[is.na(dados_cluster)] <- 0
# Normaliza
dados_cluster_scaled <- scale(dados_cluster)
# Agora roda o cotovelo
fviz_nbclust(dados_cluster_scaled, kmeans, method = "wss", k.max = 6) +
theme_minimal()
# Padroniza as variáveis (média = 0, desvio padrão = 1)
dados_cluster_scaled <- scale(dados_cluster)
summary(is.na(dados_cluster_scaled))
any(is.nan(dados_cluster_scaled))
any(is.infinite(dados_cluster_scaled))
# 1. Remove colunas com variância zero
variancias <- apply(dados_cluster, 2, var, na.rm = TRUE)
dados_cluster <- dados_cluster[, variancias > 0]
# 2. Substitui NA/NaN por média da coluna (mais estável que colocar 0)
dados_cluster <- dados_cluster %>%
mutate(across(everything(),
~ ifelse(is.na(.) | is.nan(.), mean(., na.rm = TRUE), .)))
dados_cluster <- dados_cluster[, variancias > 0]
# 3. Escala os dados
dados_cluster_scaled <- scale(dados_cluster)
# 4. Agora o método do cotovelo deve rodar sem erro
fviz_nbclust(dados_cluster_scaled, kmeans, method = "wss", k.max = 6) +
theme_minimal()
colnames(indicadores)
indicadores2 <- indicadores[,-c("retorno_diario_medio")]
indicadores2 <- indicadores %>% select(-`retorno_diario_medio`)
View(indicadores2)
# Seleciona apenas as colunas numéricas dos indicadores
dados_cluster <- indicadores2 %>%
select(retorno_diario_medio, retorno_1m, retorno_1a, retorno_2a,
vol_1m, vol_1a, vol_2a, pl_medio)
# Seleciona apenas as colunas numéricas dos indicadores
dados_cluster <- indicadores2 %>%
select(retorno_1m, retorno_1a, retorno_2a,
vol_1m, vol_1a, vol_2a, pl_medio)
# Padroniza as variáveis (média = 0, desvio padrão = 1)
dados_cluster_scaled <- scale(dados_cluster)
# Define número de clusters (k) - você pode usar o método do cotovelo
fviz_nbclust(dados_cluster_scaled, kmeans, method = "wss") +
theme_minimal()
install.packages("dbscan")
library(dbscan)
db <- dbscan(dados_cluster_scaled, eps = 0.5, minPts = 5)
dados_cluster_scaled <- na.omit(dados_cluster_scaled)
db <- dbscan(dados_cluster_scaled, eps = 0.5, minPts = 5)
indicadores$cluster_dbscan <- as.factor(db$cluster)
# Seleciona apenas as colunas numéricas dos indicadores
dados_cluster <- indicadores2 %>%
select(retorno_1m, retorno_1a, retorno_2a,
vol_1m, vol_1a, vol_2a, pl_medio)
# Padroniza as variáveis (média = 0, desvio padrão = 1)
dados_cluster_scaled <- scale(dados_cluster)
dados_cluster_scaled %>% mutate(across(everything(), ~ ifelse(is.na(.), 0, .)))
dados_cluster_scaled[is.na(dados_cluster_scaled)] <- 0
db <- dbscan(dados_cluster_scaled, eps = 0.5, minPts = 5)
indicadores$cluster_dbscan <- as.factor(db$cluster)
fviz_cluster(list(data = dados_cluster_scaled, cluster = db$cluster))
# Define número de clusters (k) - você pode usar o método do cotovelo
fviz_nbclust(dados_cluster_scaled, kmeans, method = "wss") +
theme_minimal()
# Supondo que o "cotovelo" mostrou k = 3
set.seed(123) # garante reprodutibilidade
kmeans_result <- kmeans(dados_cluster_scaled, centers = 3, nstart = 25)
# Adiciona o cluster no objeto indicadores
indicadores$cluster <- as.factor(kmeans_result$cluster)
# Visualiza os clusters em 2D (redução com PCA)
fviz_cluster(kmeans_result, data = dados_cluster_scaled,
geom = "point", ellipse.type = "norm") +
theme_minimal()
# Quantos fundos em cada cluster
table(indicadores$cluster)
# Elbow method
fviz_nbclust(dados_cluster_scaled, kmeans, method = "wss", k.max = 10) +
theme_minimal() +
ggtitle("Elbow Method: escolha do número ideal de clusters")
kmeans_result <- kmeans(dados_cluster_scaled, centers = 4, nstart = 25)
# Adiciona o cluster no objeto indicadores
indicadores$cluster <- as.factor(kmeans_result$cluster)
# Visualiza os clusters em 2D (redução com PCA)
fviz_cluster(kmeans_result, data = dados_cluster_scaled,
geom = "point", ellipse.type = "norm") +
theme_minimal()
# Define número de clusters (k) - você pode usar o método do cotovelo
fviz_nbclust(dados_cluster_scaled, kmeans, method = "wss") +
theme_minimal()
# VALIDAÇÃO: SILHOUETTE
# Supondo k = 3
set.seed(123)
kmeans_result <- kmeans(dados_cluster_scaled, centers = 3, nstart = 25)
# Calcula silhouette
sil <- silhouette(kmeans_result$cluster, dist(dados_cluster_scaled))
fviz_silhouette(sil) + ggtitle("Silhouette plot")
mean(sil[, 3])  # valor médio do silhouette (quanto mais próximo de 1, melhor)
mean(sil[, 4])  # valor médio do silhouette (quanto mais próximo de 1, melhor)
kmeans_result <- kmeans(dados_cluster_scaled, centers = 4, nstart = 25)
# Calcula silhouette
sil <- silhouette(kmeans_result$cluster, dist(dados_cluster_scaled))
fviz_silhouette(sil) + ggtitle("Silhouette plot")
# Visualiza os clusters em 2D (redução com PCA)
fviz_cluster(kmeans_result, data = dados_cluster_scaled,
geom = "point", ellipse.type = "norm") +
theme_minimal()
# Estatísticas médias por cluster
indicadores %>%
group_by(cluster) %>%
summarise(across(where(is.numeric), mean, na.rm = TRUE))
db <- dbscan(dados_cluster_scaled, eps = 0.5, minPts = 5)
indicadores$cluster_dbscan <- as.factor(db$cluster)
fviz_cluster(list(data = dados_cluster_scaled, cluster = db$cluster))
# Quantos fundos em cada cluster
table(indicadores$cluster)
# Estatísticas médias por cluster
perfil_clusters <- indicadores %>%
group_by(cluster) %>%
summarise(
retorno_medio = mean(retorno_diario_medio, na.rm = TRUE),
retorno_1m = mean(retorno_1m, na.rm = TRUE),
retorno_1a = mean(retorno_1a, na.rm = TRUE),
vol_1a = mean(vol_1a, na.rm = TRUE),
pl_medio = mean(pl_medio, na.rm = TRUE),
n_fundos = n()
)
print(perfil_clusters)
####### PACOTES ################################################################
library(lubridate) #pacote para trabalhar com datas
library(dplyr)  #pacote para tratamento de dados
library(cluster) #CLUSTERIZAÇÃO
library(factoextra)
library(dbscan)
################################################################################
####### COLETANDO OS DADOS MENSAIS E EMPILHANDO ################################
data_final <- Sys.Date() #usa a data do sistema como referência
datas <- seq(data_final %m-% months(23), data_final, by = "month") #cria uma sequência de 24 meses, terminando no mês atual
refs <- format(datas, "%Y%m") #converte de "yyyy-mm-dd" para "yyyymm"
dados_lista <- list() #lista para guardar os arquivos csv
#looping para baixar e descompactar os arquivos
for (ref in refs){ #realiza os comandos abaixo para cada data no objeto refs
url <- paste0("https://dados.cvm.gov.br/dados/FI/DOC/INF_DIARIO/DADOS/inf_diario_fi_", ref, ".zip") #cola a data no link para download
arq_zip <- tempfile(fileext = ".zip") #arquivo temporário para o compactado
arq_dir <- tempdir() #arquivo temporário para o descompactado
download.file(url, arq_zip, mode = "wb", quiet = TRUE) #baixando a url criada no arquivo temporário
unzip(arq_zip, exdir = arq_dir) #descompactando o arquivo temporário
arq_csv <- list.files(arq_dir, pattern = paste0("inf_diario_fi_", ref, ".csv$"), full.names = TRUE) #dentro do arquivo descompactado procura aquele que possui esse padrão
dados <- read.csv(arq_csv, sep = ";", stringsAsFactors = FALSE) #lê o arquivo zipado
#tratando os meses em que as colunas estão com nomes diferentes
if ("TP_FUNDO_CLASSE" %in% names(dados)) {
names(dados)[names(dados) == "TP_FUNDO_CLASSE"] <- "TP_FUNDO"
}
#tratando os meses em que as colunas estão com nomes diferentes
if ("CNPJ_FUNDO_CLASSE" %in% names(dados)) {
names(dados)[names(dados) == "CNPJ_FUNDO_CLASSE"] <- "CNPJ_FUNDO"
}
dados$referencia <- ref #criando uma coluna com o mês de referência
dados_lista[[ref]] <- dados #incluíndo os dados na lista
}
dados_completos <- bind_rows(dados_lista) #empilhando os dados da lista usando dplyr
################################################################################
####### ESTATÍSTICAS DA BASE DE DADOS COMPLETA #################################
#quantidade de fundos na base em todo o período (sem filtrar) = 31555
length(unique(dados_completos$CNPJ_FUNDO)) #tamanho do vetor de cnpj (sem repetições)
################################################################################
####### FILTRANDO A BASE DE DADOS ##############################################
### FILTRANDO FUNDOS QUE OPERARAM NO ÚLTIMO ANO
#coluna data está com classe de caracteres, deve-se transformar em data
class(dados_completos$DT_COMPTC)
#transformar a coluna de data em Date para conseguir manipular
dados_completos$DT_COMPTC <- as.Date(dados_completos$DT_COMPTC)
#calcular a data de corte = 365 dias corridos a menos
data_corte <- max(dados_completos$DT_COMPTC) %m-% days(365)
#fundos que possuem pelo menos 252 observações (dias úteis) desde a data de corte
cnpjs_filtrados1 <- dados_completos %>%
filter(DT_COMPTC >= data_corte) %>%
group_by(CNPJ_FUNDO) %>%
summarise(n_obs = sum(!is.na(VL_QUOTA))) %>%
filter(n_obs >= 252) %>%
pull(CNPJ_FUNDO)
#### FILTRANDO CNPJS COM MAIS DE 100 COTISTAS (RETIRA FUNDOS EXCLUSIVOS E POTENCIALMENTE FECHADOS)
cnpjs_filtrados2 <- dados_completos %>% #esse símbolo significa que vai fazer uma sequência de operações
group_by(CNPJ_FUNDO) %>% #opera por CNPJ
summarise(media_cotistas = mean(NR_COTST, na.rm = TRUE)) %>% #tira a média de cotistas no período por CNPJ
filter(media_cotistas > 100)  %>% #filtra apenas CNPJs com mais de 100 cotistas em média
pull(CNPJ_FUNDO) #puxa apenas o cnpj no objeto final
#aplicando os filtros
dados_filtrados <- dados_completos %>%
filter(CNPJ_FUNDO %in% cnpjs_filtrados1,
CNPJ_FUNDO %in% cnpjs_filtrados2)
#quantidade de fundos na base filtrada = 1029
length(unique(dados_filtrados$CNPJ_FUNDO))
#estatisticas descritivas
################################################################################
####### CALCULANDO INDICADORES #################################################
indicadores <- dados_filtrados %>%
arrange(CNPJ_FUNDO, DT_COMPTC) %>% #organiza por CNPJ e data
group_by(CNPJ_FUNDO) %>% #realizar as operações abaixo para cada CNPJ
mutate( #cria colunas
retorno_diario = (VL_QUOTA / lag(VL_QUOTA)) - 1,
ano_atual = year(DT_COMPTC),
mes_atual = floor_date(DT_COMPTC, "month")
) %>%
summarise( #cria os indicadores para cada CNPJ
#retorno mediário médio no período inteiro
retorno_diario_medio = 100*mean(retorno_diario, na.rm = TRUE),
#retorno no último mês
retorno_1m = 100*(
last(VL_QUOTA, order_by = DT_COMPTC)/
first(na.omit(VL_QUOTA[DT_COMPTC>=(max(DT_COMPTC) %m-% months(1))]))-1),
#retorno no último ano
retorno_1a = 100*(
last(VL_QUOTA, order_by = DT_COMPTC)/
first(na.omit(VL_QUOTA[DT_COMPTC >= (max(DT_COMPTC) %m-% years(1))]))-1),
#retorno nos últimos 2 anos
retorno_2a = 100*(
last(VL_QUOTA, order_by = DT_COMPTC)/
first(na.omit(VL_QUOTA), order_by = DT_COMPTC)-1),
#volatilidade no último mês anualizada
vol_1m = sd(retorno_diario[DT_COMPTC >= (max(DT_COMPTC) %m-% months(1))],na.rm = TRUE)*sqrt(252),
#volatilidade no último ano anualizada
vol_1a=sd(retorno_diario[DT_COMPTC>=(max(DT_COMPTC) %m-% years(1))],na.rm = TRUE)*sqrt(252),
#volatilidade nos últimos 2 anos anualizada
vol_2a=sd(retorno_diario, na.rm=TRUE)*sqrt(2*252),
#patrimônio líquido médio
pl_medio = mean(VL_PATRIM_LIQ, na.rm = TRUE)
)
View(indicadores)
#fundo com maior retorno no último mês
head(indicadores %>% arrange(desc(retorno_1m)),1)
#fundo com menor retorno no último mês
head(indicadores %>% arrange(retorno_1m),1)
#fundo com menor volatilidade no último ano
head(indicadores %>% arrange(vol_1a),1)
#fundo com menor volatilidade no último ano
head(indicadores %>% arrange(desc(vol_1a)),1) #retornos bem menores que o fundo com menos vol
#resumo dos indicadores
summary(indicadores)
################################################################################
####### CLUSTERIZAÇÃO ##########################################################
indicadores2 <- indicadores %>% select(-`retorno_diario_medio`)
# Seleciona apenas as colunas numéricas dos indicadores
dados_cluster <- indicadores2 %>%
select(retorno_1m, retorno_1a, retorno_2a,
vol_1m, vol_1a, vol_2a, pl_medio)
# Padroniza as variáveis (média = 0, desvio padrão = 1)
dados_cluster_scaled <- scale(dados_cluster)
dados_cluster_scaled[is.na(dados_cluster_scaled)] <- 0 #SUBSTUTINDO NA POR 0
### KMEANS
# VALIDAÇÃO: ELBOW
fviz_nbclust(dados_cluster_scaled, kmeans, method = "wss") +
theme_minimal()
# Supondo que o "cotovelo" mostrou k = 3
set.seed(123) # garante reprodutibilidade
kmeans_result <- kmeans(dados_cluster_scaled, centers = 4, nstart = 25)
# VALIDAÇÃO: SILHOUETTE
# Supondo k = 4
# Calcula silhouette
sil <- silhouette(kmeans_result$cluster, dist(dados_cluster_scaled))
fviz_silhouette(sil) + ggtitle("Silhouette plot")
mean(sil[, 3])  # valor médio do silhouette (quanto mais próximo de 1, melhor)
# Adiciona o cluster no objeto indicadores
indicadores$cluster <- as.factor(kmeans_result$cluster)
# Visualiza os clusters em 2D (redução com PCA)
fviz_cluster(kmeans_result, data = dados_cluster_scaled,
geom = "point", ellipse.type = "norm") +
theme_minimal()
# Quantos fundos em cada cluster
table(indicadores$cluster)
# Estatísticas médias por cluster
# Adiciona a coluna do cluster no objeto indicadores
indicadores$cluster <- as.factor(kmeans_result$cluster)
# Estatísticas médias por cluster
perfil_clusters <- indicadores %>%
group_by(cluster) %>%
summarise(
retorno_medio = mean(retorno_diario_medio, na.rm = TRUE),
retorno_1m = mean(retorno_1m, na.rm = TRUE),
retorno_1a = mean(retorno_1a, na.rm = TRUE),
vol_1a = mean(vol_1a, na.rm = TRUE),
pl_medio = mean(pl_medio, na.rm = TRUE),
n_fundos = n()
)
print(perfil_clusters)
### DBSCAN
db <- dbscan(dados_cluster_scaled, eps = 0.5, minPts = 5)
indicadores$cluster_dbscan <- as.factor(db$cluster)
fviz_cluster(list(data = dados_cluster_scaled, cluster = db$cluster))
################################################################################
